# Ollama Web Interface

A modern web interface for managing and interacting with Ollama models on localhost.

## Features

- üîÑ **Monitor Running Models** - View all running models with resource usage and auto-unload timers
- ‚¨áÔ∏è **Pull Models** - Download models with real-time progress bars
- üì¶ **Manage Models** - List all available models with size and modification info
- üí¨ **Chat Interface** - Test models with a simple chat interface
- üõ†Ô∏è **Model Operations**:
  - Copy models
  - Rename models (copy + remove)
  - Remove models
  - Show detailed model information
  - Stop running models

## Installation

### Via Pinokio (Recommended)

1. Clone or place this repository in your Pinokio API directory: `C:\pinokio\api\Ollama_Web.git\`
2. Open Pinokio and find "Ollama Web Interface" in your apps
3. Click "Install" to set up the environment
4. Click "Start" to launch the web interface

### Manual Installation

1. Ensure Python 3.8+ is installed
2. Create a virtual environment:
   ```bash
   python -m venv venv
   ```
3. Activate the virtual environment:
   - Windows: `venv\Scripts\activate`
   - Linux/Mac: `source venv/bin/activate`
4. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

## Usage

### Start the Server

**Via Pinokio:** Click the "Start" button in Pinokio

**Manually:**
```bash
python app.py
```

The web interface will be available at: `http://localhost:11435`

### Prerequisites

- Ollama must be installed and running on `localhost:11434`
- Download Ollama from: https://ollama.ai

### Available Ollama Commands

The interface supports all major Ollama operations:

- **serve** - Start ollama (if not already running)
- **pull** - Download models from registry with progress tracking
- **list** - View all installed models
- **ps** - Monitor running models with resource info
- **run** - Chat with models through the interface
- **stop** - Stop running models
- **cp** - Copy models
- **rm** - Remove models
- **show** - Display detailed model information

## Project Structure

```
Ollama_Web.git/
‚îú‚îÄ‚îÄ app.py              # Flask application
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html      # Web interface
‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îú‚îÄ‚îÄ pinokio.js         # Pinokio configuration
‚îú‚îÄ‚îÄ install.json       # Pinokio install script
‚îú‚îÄ‚îÄ start.json         # Pinokio start script
‚îú‚îÄ‚îÄ stop.json          # Pinokio stop script
‚îú‚îÄ‚îÄ update.json        # Pinokio update script
‚îî‚îÄ‚îÄ reset.json         # Pinokio reset script
```

## API Endpoints

- `GET /api/ps` - List running models
- `GET /api/list` - List all models
- `POST /api/pull` - Pull a model
- `GET /api/pull/progress/<model>` - Get pull progress
- `POST /api/stop` - Stop a running model
- `POST /api/rm` - Remove a model
- `POST /api/cp` - Copy a model
- `POST /api/rename` - Rename a model
- `POST /api/show` - Show model info
- `POST /api/chat` - Chat with a model

## Configuration

The server runs on `0.0.0.0:11435` by default and connects to Ollama at `localhost:11434`.

To change these settings, edit `app.py`:

```python
OLLAMA_URL = "http://localhost:11434"
app.run(host='0.0.0.0', port=11435)
```

## License

MIT

## Credits

Built for use with [Ollama](https://ollama.ai) and [Pinokio](https://pinokio.computer)
